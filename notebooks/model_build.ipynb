{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import toml\n",
    "with open(\"../config.toml\", \"r\") as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "DATA_FOLDER = config[\"DATA_FOLDER\"]\n",
    "MODEL_FOLDER = config[\"MODEL_FOLDER\"]\n",
    "\n",
    "random_seed = 46\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "jobs = -1 #this is the number of cores that the models and test will run on. -1 means that all cores will be used \n",
    "\n",
    "#dropping all of the observations that are very likely errors\n",
    "dataset = pd.read_csv(DATA_FOLDER + \"/master_dataset.csv\")\n",
    "dataset = dataset[dataset[\"cases_hrs\"] <= 300]\n",
    "dataset = dataset[dataset[\"Total_Hours\"] >= 10]\n",
    "\n",
    "\n",
    "#dropping all of the uneeded columns\n",
    "to_drop = [\"Date\", \"Total_Hours\", \"Total_Cases\", \"B_HrsPct\", \"B_Cases\", \"Total_Each_Day\", \"dry_ratio\", \"clr_ratio\", \"frz_ratio\", \"GO_LIVE_DATE\", \"LABEL_TYPE\"]\n",
    "dataset_build = dataset.drop(labels=to_drop, axis=1)\n",
    "dataset_build.rename(columns={\"BRNCH_CD\":\"brnch_cd\", \"A_HrsPct\":\"a_hrspct\", \"C_HrsPct\":\"c_hrspct\", \"A_Cases\":\"a_cases\", \"C_Cases\":\"c_cases\"}, inplace=True)\n",
    "dataset_build.to_csv(DATA_FOLDER + \"/model_dataset.csv\",index=False)\n",
    "\n",
    "numeric_features = [\"a_hrspct\",  'c_hrspct', 'a_cases', 'c_cases']\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())]\n",
    ")\n",
    "categorical_features=['brnch_cd', 'weekday', 'month']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_build.drop(labels=\"cases_hrs\", axis=1),dataset_build['cases_hrs'], random_state=random_seed, train_size = .70)\n",
    "\n",
    "scores = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1         942.9804            4.58m\n",
      "         2         758.2974            4.61m\n",
      "         3         631.9937            4.54m\n",
      "         4         554.0886            4.55m\n",
      "         5         501.9105            4.52m\n",
      "         6         456.6620            4.45m\n",
      "         7         424.0542            4.39m\n",
      "         8         397.7424            4.30m\n",
      "         9         364.3594            4.22m\n",
      "        10         346.8073            4.18m\n",
      "        20         234.1818            3.84m\n",
      "        30         186.0763            3.71m\n",
      "        40         153.4382            3.61m\n",
      "        50         129.5386            3.55m\n",
      "        60         107.3654            3.51m\n",
      "        70          95.1016            3.46m\n",
      "        80          84.3506            3.42m\n",
      "        90          75.3225            3.38m\n",
      "       100          66.8518            3.34m\n",
      "       200          31.3407            2.96m\n",
      "       300          20.3148            2.56m\n",
      "       400          15.1142            2.17m\n",
      "       500          12.5242            1.80m\n",
      "       600          11.1673            1.43m\n",
      "       700          10.3272            1.07m\n",
      "       800           9.8351           42.73s\n",
      "       900           9.5912           21.30s\n",
      "      1000           9.4635            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1039.3215            1.81m\n",
      "         2         905.5189            1.82m\n",
      "         3         808.6662            1.77m\n",
      "         4         741.5897            1.80m\n",
      "         5         694.4403            1.79m\n",
      "         6         657.9652            1.78m\n",
      "         7         625.6210            1.75m\n",
      "         8         599.4991            1.73m\n",
      "         9         577.2915            1.71m\n",
      "        10         556.8384            1.70m\n",
      "        20         442.8154            1.62m\n",
      "        30         382.6709            1.58m\n",
      "        40         345.9055            1.56m\n",
      "        50         311.9708            1.53m\n",
      "        60         295.0574            1.51m\n",
      "        70         277.3263            1.50m\n",
      "        80         265.8632            1.48m\n",
      "        90         252.1486            1.47m\n",
      "       100         241.9686            1.45m\n",
      "       200         178.4118            1.30m\n",
      "       300         148.0572            1.14m\n",
      "       400         126.7578           58.94s\n",
      "       500         110.3118           49.21s\n",
      "       600          97.3869           39.50s\n",
      "       700          87.3641           29.69s\n",
      "       800          78.9401           19.80s\n",
      "       900          70.9295            9.90s\n",
      "      1000          64.5161            0.00s\n"
     ]
    }
   ],
   "source": [
    "#linear regression\n",
    "reg_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", LinearRegression())]\n",
    ")\n",
    "reg_pipe.fit(X_train, y_train)\n",
    "scores.append([\"Reg\", reg_pipe.score(X_test, y_test)])\n",
    "\n",
    "#gradient boosting regressor model\n",
    "gbr_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(random_state=random_seed))]\n",
    ")\n",
    "gbr_pipe.fit(X_train, y_train)\n",
    "scores.append([\"GBR Init\", gbr_pipe.score(X_test, y_test)])\n",
    "\n",
    "#improved gbr\n",
    "gbr_improved_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, n_estimators=1000, learning_rate=0.3, max_depth=10, random_state=random_seed, loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved\", gbr_improved_pipe.score(X_test, y_test)])\n",
    "\n",
    "#improved gbr 2.0 - lower depth\n",
    "gbr_improved_pipe_2 = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, n_estimators=1000, learning_rate=0.3, max_depth=5, random_state=random_seed, loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe_2.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved\", gbr_improved_pipe_2.score(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GBR Grid Search\n",
    "* Takes a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngbrgrid = {\\'n_estimators\\':[500,1000,2000], \\n           \\'learning_rate\\':[0.15,0.3], \\n           \\'max_depth\\':[5,10]}\\n\\ncrossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\\n\\ngbrgrid_pipe = Pipeline(\\n    steps=[(\"preprocessor\", preprocessor), (\"gbrsearch\", GridSearchCV(estimator = GradientBoostingRegressor(), \\n                                                                      param_grid = gbrgrid,\\n                                                                      scoring = \\'neg_mean_squared_error\\',\\n                                                                      verbose = 1,\\n                                                                      cv = crossvalidation, n_jobs=jobs))]\\n)\\ngbrsearch = gbrgrid_pipe.fit(X_train, y_train)\\ngbrgrid_pipe.best_params_, gbrgrid_pipe.best_score_\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "gbrgrid = {'n_estimators':[500,1000,2000], \n",
    "           'learning_rate':[0.15,0.3], \n",
    "           'max_depth':[5,10]}\n",
    "\n",
    "crossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\n",
    "\n",
    "gbrgrid_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"gbrsearch\", GridSearchCV(estimator = GradientBoostingRegressor(), \n",
    "                                                                      param_grid = gbrgrid,\n",
    "                                                                      scoring = 'neg_mean_squared_error',\n",
    "                                                                      verbose = 1,\n",
    "                                                                      cv = crossvalidation, n_jobs=jobs))]\n",
    ")\n",
    "gbrsearch = gbrgrid_pipe.fit(X_train, y_train)\n",
    "gbrgrid_pipe.best_params_, gbrgrid_pipe.best_score_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1211.4200            4.57m\n",
      "         2        1170.0448            4.53m\n",
      "         3        1132.6770            4.48m\n",
      "         4        1097.3747            4.49m\n",
      "         5        1063.9353            4.49m\n",
      "         6        1033.3433            4.50m\n",
      "         7        1004.9712            4.51m\n",
      "         8         978.5187            4.53m\n",
      "         9         954.0518            4.53m\n",
      "        10         929.0276            4.52m\n",
      "        20         760.7208            4.47m\n",
      "        30         661.8552            4.45m\n",
      "        40         595.0819            4.37m\n",
      "        50         546.9083            4.27m\n",
      "        60         512.4975            4.17m\n",
      "        70         485.4619            4.10m\n",
      "        80         462.8035            4.04m\n",
      "        90         443.5262            3.98m\n",
      "       100         426.0728            3.94m\n",
      "       200         323.1951            3.60m\n",
      "       300         275.5669            3.35m\n",
      "       400         245.5127            3.14m\n",
      "       500         223.2356            2.94m\n",
      "       600         205.7694            2.74m\n",
      "       700         191.9672            2.55m\n",
      "       800         180.4473            2.36m\n",
      "       900         169.8034            2.16m\n",
      "      1000         161.1831            1.96m\n",
      "      2000         104.7418            0.00s\n"
     ]
    }
   ],
   "source": [
    "#improved gbr\n",
    "    ## to reduce variablity, depth for gbr should be between 4 - 8\n",
    "    ## n_estimators = 2000\n",
    "    ## smaller learning rate = 0.05 - 0.15 is good\n",
    "    ## 4 minutes\n",
    "gbr_improved_pipe_3 = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, \n",
    "                                                                               n_estimators=2000, \n",
    "                                                                               learning_rate=0.05,\n",
    "                                                                               max_depth=6,  \n",
    "                                                                               loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe_3.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved 3\",gbr_improved_pipe_3.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Trees Regressor\n",
    "* Best MSE / RMSE = 242.242 / 15.564\n",
    "    * With DEFAULT paramters, performs pretty well against tuned GBR\n",
    "    * increasing n_estimators does not change much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   57.2s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "#Extra trees base model, 5 1/2 minutes\n",
    "xtratree_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", ExtraTreesRegressor(verbose=1, n_jobs = jobs))]\n",
    ")\n",
    "xtratree_pipe.fit(X_train, y_train)\n",
    "scores.append([\"xtratree\",xtratree_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "* Best MSE / RMSE = 261.028 / 16.156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncrossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\\nrandfor_param = {\\n             'max_depth': [5, 10, 15],\\n             'n_estimators': [500, 1000, 1500]}\\nrandfor_search = GridSearchCV(RandomForestRegressor(n_jobs=jobs), randfor_param, refit = True, verbose = 3, cv = crossvalidation, scoring = 'neg_mean_squared_error', n_jobs=jobs)\\nrandfor_search.fit(X_train, y_train)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest grid search\n",
    "## run if needed\n",
    "\"\"\"\n",
    "crossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\n",
    "randfor_param = {\n",
    "             'max_depth': [5, 10, 15],\n",
    "             'n_estimators': [500, 1000, 1500]}\n",
    "randfor_search = GridSearchCV(RandomForestRegressor(n_jobs=jobs), randfor_param, refit = True, verbose = 3, cv = crossvalidation, scoring = 'neg_mean_squared_error', n_jobs=jobs)\n",
    "randfor_search.fit(X_train, y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   48.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#baseline RFR, outperformed by extratrees regressor - 4 min\n",
    "randfor_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", RandomForestRegressor(verbose=1, n_jobs=jobs))]\n",
    ")\n",
    "randfor_pipe.fit(X_train, y_train)\n",
    "scores.append([\"RFR\", randfor_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressor\n",
    "* Best MSE / RMSE = 466.094 / 21.589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#baseline Decision tree regressor\n",
    "dectree_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", DecisionTreeRegressor())]\n",
    ")\n",
    "dectree_pipe.fit(X_train, y_train)\n",
    "scores.append([\"Decision Tree\", dectree_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Reg', 0.6580482257434541],\n",
       " ['GBR Init', 0.6106992906929891],\n",
       " ['GBR Improved', 0.7829166031320501],\n",
       " ['GBR Improved', 0.7849626923522042],\n",
       " ['GBR Improved 3', 0.795639625461525],\n",
       " ['xtratree', 0.7945109664434109],\n",
       " ['RFR', 0.7812300991781476],\n",
       " ['Decision Tree', 0.6153347876716004]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../src/models/xtra_tree.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pickling the model \n",
    "from joblib import dump\n",
    "\n",
    "dump(reg_pipe, MODEL_FOLDER + \"/lin_reg.joblib\") \n",
    "dump(gbr_pipe, MODEL_FOLDER + \"/gbr_init.joblib\")\n",
    "dump(gbr_improved_pipe, MODEL_FOLDER + \"/gbr_improved.joblib\")\n",
    "dump(gbr_improved_pipe_2, MODEL_FOLDER + \"/gbr_improved_2.joblib\")\n",
    "dump(gbr_improved_pipe_3, MODEL_FOLDER +\"/gbr_improved_3.joblib\")\n",
    "dump(randfor_pipe, MODEL_FOLDER + \"/rfr.joblib\")\n",
    "dump(dectree_pipe, MODEL_FOLDER + \"/dec_tree.joblib\")\n",
    "dump(xtratree_pipe, MODEL_FOLDER + \"/xtra_tree.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
