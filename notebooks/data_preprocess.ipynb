{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e2e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file used to generate outputs for presentations\n",
    "import warnings\n",
    "#use this when printing the output to a pdf\n",
    "warnings.filterwarnings('ignore')\n",
    "#use this when doing coding normally\n",
    "#warnings.filterwarnings('default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9279b829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sys import path\n",
    "path.append(\"..\")\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24d52ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = pd.read_csv(DATA_FOLDER + \"/sae.csv\")\n",
    "#cases = pd.read_csv(\"data/Cases.csv\")\n",
    "#hours = pd.read_csv(\"data/hours.csv\")\n",
    "cases = pd.read_csv(DATA_FOLDER + \"/random_cases.csv\")\n",
    "hours = pd.read_csv(DATA_FOLDER + \"/random_hours.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "311a1476",
   "metadata": {},
   "source": [
    "Merging hours and cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3fdaea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'src/datasae.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"# Cases\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m sae \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(DATA_FOLDER \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msae.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m \u001b[39m#cases = pd.read_csv(\"data/Cases.csv\")\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m#hours = pd.read_csv(\"data/hours.csv\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m cases \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(DATA_FOLDER \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrandom_cases.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/gits/daily-productivity-models/.venv/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/gits/daily-productivity-models/.venv/lib/python3.11/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/gits/daily-productivity-models/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/gits/daily-productivity-models/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/gits/daily-productivity-models/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/gits/daily-productivity-models/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1730\u001b[0m     f,\n\u001b[1;32m   1731\u001b[0m     mode,\n\u001b[1;32m   1732\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1733\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1734\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1735\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1736\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1737\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1738\u001b[0m )\n\u001b[1;32m   1739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/gits/daily-productivity-models/.venv/lib/python3.11/site-packages/pandas/io/common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    856\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    858\u001b[0m             handle,\n\u001b[1;32m    859\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    860\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    861\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    862\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    863\u001b[0m         )\n\u001b[1;32m    864\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    865\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'src/datasae.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"# Cases\n",
    "\n",
    "\n",
    "* Change date to date_time object\n",
    "    * Create a new Date column for both datasets\n",
    "    * Allows merging on shared column\n",
    "*  Sort dates in ascending order\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cases\n",
    "\n",
    "# check type, length\n",
    "cases.CLNDR_DT\n",
    "\n",
    "#change CLNDR_DT from object to date_time\n",
    "  # make sure to create new column 'Date' for both datasets so they share common \n",
    "  # for merging\n",
    "\n",
    "cases['Date'] = pd.to_datetime(cases['CLNDR_DT'])\n",
    "cases\n",
    "\n",
    "#sort dates from past to current\n",
    "cases = cases.sort_values(by='CLNDR_DT')\n",
    "cases\n",
    "\n",
    "#drop redundant columns\n",
    "\n",
    "#^Dont need this line\n",
    "#cases = cases.drop(columns=['CLNDR_DT', 'DIV_NM'])\n",
    "#cases\n",
    "\n",
    "# check number of NaN values\n",
    "cases.isna().sum()\n",
    "\n",
    "## needed to drop NaN in order for 'tot_each_day' to be correct number\n",
    "    ## ask why including NaN would yield wrong sum\n",
    "        ## what values does NaN have?\n",
    "\n",
    "cases = cases.dropna()\n",
    "cases\n",
    "\n",
    "## create new dataframe from cases\n",
    "    ## focusing on getting total of eaches selected per day\n",
    "cases_tot_day = pd.DataFrame(cases)\n",
    "\n",
    "## group by Date and BRNCH_CD, sum eaches_selected by each zone into one total each column\n",
    "cases_tot_day['Total_Each_Day'] = pd.DataFrame(cases.groupby(['Date', 'BRNCH_CD'])['EACHES_SELECTED'].transform('sum'))\n",
    "cases_tot_day\n",
    "\n",
    "## New Dataframe with order = Date, Branch, zone, eaches selected, total eaches\n",
    "cases4 = pd.DataFrame(cases, columns = (['Date', 'BRNCH_CD', 'ZONE', 'EACHES_SELECTED', 'Total_Each_Day']))\n",
    "\n",
    "## Change index starting at 1, drop existing index to avoid adding as column\n",
    "cases4 = cases4.reset_index(drop=True)\n",
    "\n",
    "cases4.BRNCH_CD[0]\n",
    "\n",
    "## get ratio of total eaches per zone\n",
    "    ## eaches selected of zone / total eaches\n",
    "    ## add ratio to a list and append to dataset after\n",
    "\n",
    "ratio = []\n",
    "for i in range(0, len(cases4)):\n",
    "  #print(i)\n",
    "  r = cases4.EACHES_SELECTED[i] / cases4.Total_Each_Day[i]\n",
    "  ratio.append(r)\n",
    "\n",
    "ratio[0:10]\n",
    "\n",
    "## add list of ratios to dataframe, should be in order\n",
    "cases4['ratio'] = ratio\n",
    "\n",
    "#type(cases4.ZONE[1])\n",
    "\n",
    "## create a list for every zone (dry, clr, frz)\n",
    "## for every row in dataframe:\n",
    "    ## if the zone of the row == \"DRY\":\n",
    "        ## add the ratio at index i to the \"DRY\" list\n",
    "        ## add zeros to the other lists (ClR, FRZ) (place holder)\n",
    "    ## if zone of row == \"CLR\":\n",
    "        ## add ratio to CLR list\n",
    "        ## add zeros to other lists\n",
    "    ## repeat for FRZ\n",
    "\n",
    "## add lists as columns to dataset\n",
    "\n",
    "dry_ratio = []\n",
    "clr_ratio = []\n",
    "frz_ratio = []\n",
    "\n",
    "for i in range(0, len(cases4)): \n",
    "  if cases4.ZONE[i] == 'DRY':\n",
    "    dry_ratio.append(cases4.ratio[i])\n",
    "    clr_ratio.append(0)\n",
    "    frz_ratio.append(0)\n",
    "  elif cases4.ZONE[i] == 'FRZ':\n",
    "    dry_ratio.append(0)\n",
    "    clr_ratio.append(0)\n",
    "    frz_ratio.append(cases4.ratio[i])\n",
    "  elif cases4.ZONE[i] == 'CLR':\n",
    "    dry_ratio.append(0)\n",
    "    clr_ratio.append(cases4.ratio[i])\n",
    "    frz_ratio.append(0)\n",
    "\n",
    "\n",
    "cases4['dry_ratio'] = dry_ratio\n",
    "cases4['clr_ratio'] = clr_ratio\n",
    "cases4['frz_ratio'] = frz_ratio\n",
    "\n",
    "## sort dataframe by date and branch to see everything together\n",
    "    ## check if math is correct\n",
    "cases4 = cases4.sort_values(by=['Date', 'BRNCH_CD'], ascending = [True, True])\n",
    "\n",
    "## smoosh multiple rows of same branch and date into 1 row\n",
    "    ## aggregate sums, zeros as placeholders will have no effect on ratio\n",
    "cases6 = cases4.groupby(['Date', 'BRNCH_CD', 'Total_Each_Day'], as_index = False).agg('sum')\n",
    "\n",
    "\n",
    "## drop eaches since same as total\n",
    "## drop ratio since all ratios add up to 1 anyway\n",
    "cases_EachPerDay = cases6.drop(columns=['EACHES_SELECTED', 'ratio'])\n",
    "\n",
    "\"\"\"# Hours\n",
    "\n",
    "\n",
    "*   Repeat steps from cases\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "hours.REPORT_DATE\n",
    "\n",
    "hours['Date'] = pd.to_datetime(hours['REPORT_DATE'])\n",
    "\n",
    "hours = hours.sort_values(by=['REPORT_DATE', 'BRNCH_CD'], ascending = [True, True])\n",
    "\n",
    "hours = hours.drop(columns=['DIV_NBR', 'REPORT_DATE', 'FULL_MARKET_NAME'])\n",
    "\n",
    "## drop \"NaN\" (UNKOWN)\n",
    "hours = hours[hours.COHORT != 'UNKNOWN']\n",
    "\n",
    "## create datarame in order desired\n",
    "hours_co = pd.DataFrame(hours, columns = ['Date', 'BRNCH_CD', 'COHORT', 'CASES_SELECTED', 'MEASURED_DIRECT_HRS'])\n",
    "\n",
    "## group by Date and branch to get sum of HOURS MEASURED of each cohort as total hours worked at branch on specific day\n",
    "hours_co['Total_Hours'] = pd.DataFrame(hours.groupby(['Date', 'BRNCH_CD'])['MEASURED_DIRECT_HRS'].transform('sum'))\n",
    "\n",
    "\n",
    "## group by Date and branch to get sum of CASES SELECTED of each cohort as total cases selected at branch on specific day\n",
    "hours_co['Total_Cases'] = pd.DataFrame(hours.groupby(['Date', 'BRNCH_CD'])['CASES_SELECTED'].transform('sum'))\n",
    "\n",
    "\n",
    "hours_co = hours_co.reset_index(drop = True)\n",
    "\n",
    "## like hours, get ratio of hours worked per cohort relative to total hours measured at branch on specific day\n",
    "Hrs_PerCo = []\n",
    "for i in range (0, len(hours_co)):\n",
    "  r = hours_co.MEASURED_DIRECT_HRS[i] / hours_co.Total_Hours[i]\n",
    "  Hrs_PerCo.append(r)\n",
    "\n",
    "Hrs_PerCo[0:5]\n",
    "\n",
    "hours_co['Hrs_Pct'] = Hrs_PerCo\n",
    "hours_co\n",
    "\n",
    "## same process of placing respective hours ratios per cohort as eaches per zone in cases\n",
    "    ## 0.000 = placeholder for aggregation\n",
    "A_Hours = []\n",
    "B_Hours = []\n",
    "C_Hours = []\n",
    "\n",
    "for i in range(0, len(hours_co)):\n",
    "  if hours_co.COHORT[i] == 'A':\n",
    "    A_Hours.append(hours_co.Hrs_Pct[i])\n",
    "    B_Hours.append(0)\n",
    "    C_Hours.append(0)\n",
    "  elif hours_co.COHORT[i] == 'C':\n",
    "    A_Hours.append(0)\n",
    "    B_Hours.append(0)\n",
    "    C_Hours.append(hours_co.Hrs_Pct[i])\n",
    "  elif hours_co.COHORT[i] == 'B':\n",
    "    A_Hours.append(0)\n",
    "    B_Hours.append(hours_co.Hrs_Pct[i])\n",
    "    C_Hours.append(0)\n",
    "\n",
    "hours_co['A_HrsPct'] = A_Hours\n",
    "hours_co['B_HrsPct'] = B_Hours\n",
    "hours_co['C_HrsPct'] = C_Hours\n",
    "\n",
    "## ratio of cases per cohort\n",
    "Cases_PerCo = []\n",
    "for i in range (0, len(hours_co)):\n",
    "  r = hours_co.CASES_SELECTED[i] / hours_co.Total_Cases[i]\n",
    "  Cases_PerCo.append(r)\n",
    "\n",
    "hours_co['Cases_Pct'] = Cases_PerCo\n",
    "\n",
    "## cases ratio per cohort w/ placeholders\n",
    "A_Cases = []\n",
    "B_Cases = []\n",
    "C_Cases = []\n",
    "\n",
    "for i in range(0, len(hours_co)):\n",
    "  if hours_co.COHORT[i] == 'A':\n",
    "    A_Cases.append(hours_co.Cases_Pct[i])\n",
    "    B_Cases.append(0)\n",
    "    C_Cases.append(0)\n",
    "  elif hours_co.COHORT[i] == 'C':\n",
    "    A_Cases.append(0)\n",
    "    B_Cases.append(0)\n",
    "    C_Cases.append(hours_co.Cases_Pct[i])\n",
    "  elif hours_co.COHORT[i] == 'B':\n",
    "    A_Cases.append(0)\n",
    "    B_Cases.append(hours_co.Cases_Pct[i])\n",
    "    C_Cases.append(0)\n",
    "\n",
    "\n",
    "hours_co['A_Cases'] = A_Cases\n",
    "hours_co['B_Cases'] = B_Cases\n",
    "hours_co['C_Cases'] = C_Cases\n",
    "\n",
    "hours_co\n",
    "\n",
    "hours_CoHrs = hours_co.groupby(['Date', 'BRNCH_CD', 'Total_Hours', 'Total_Cases'], as_index = False).agg('sum')\n",
    "hours_CoHrs\n",
    "\n",
    "\"\"\"# New Datasets\"\"\"\n",
    "\n",
    "cases_EachPerDay\n",
    "\n",
    "## Date = CLNDR_DATE\n",
    "## BRNCH_CD = code referring to sepcific Warehouse\n",
    "\n",
    "## Total_Each_Day = sum of eaches selected by all zones on specific date\n",
    "    ## dry_ratio = proportion of dry products relative to total eaches selected on specific date \n",
    "    ## clr_ratio = proportion of cooler products relative to total eaches selected on specific date\n",
    "    ## frz_ratio = proportion of freezer products relative to total eaches selected on specific date\n",
    "\n",
    "hours_CohortHrsCases = pd.DataFrame(hours_CoHrs, columns = ['Date', 'BRNCH_CD', \n",
    "                                                            'Total_Hours', 'Total_Cases',\n",
    "                                                            'A_HrsPct',\n",
    "                                                            'B_HrsPct', 'C_HrsPct', \n",
    "                                                            'A_Cases', 'B_Cases','C_Cases' \n",
    "                                                            ])\n",
    "hours_CohortHrsCases\n",
    "\n",
    "## Date = REPORT_DATE\n",
    "## BRNCH_CD = code referring to sepcific Warehouse\n",
    "\n",
    "## Total_Hours = sum of hours worked by all cohorts on specific date\n",
    "    ## A_HrsPct = proportion of hours worked by Cohort A relative to total hours on specific date\n",
    "    ## B_HrsPct = proportion of hours worked by Cohort B relative to total hours on specific date\n",
    "    ## C_HrsPct = proportion of hours worked by Cohort C relative to total hours on specific date\n",
    "\n",
    "## Total_Cases = Sum of all cases selected by all cohorts on specific date\n",
    "    ## A_Cases = proportion of cases selected by Cohort A relative to total cases selected on specific date\n",
    "    ## B_Cases = proportion of cases selected by Cohort B relative to total cases selected on specific date\n",
    "    ## C_Cases = proportion of cases selected by Cohort C relative to total cases selected on specific date\n",
    "\n",
    "\"\"\"# Merge\n",
    "* lost a lot of data :/\n",
    "* dont merge yet?\n",
    "\"\"\"\n",
    "\n",
    "Foods = hours_CohortHrsCases.merge(cases_EachPerDay, how = 'inner', on = ['Date', 'BRNCH_CD'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8965090",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "Adding dates to the dataset and merging merged dataset with the sae dataset. Also created dummy variables for the categorical variables (month, weekday, branch_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb54248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Foods2 = Foods\n",
    "\n",
    "#adding the response variable(s) to the dataset\n",
    "Foods2[\"cases_hrs\"] = Foods2['Total_Cases']/Foods2['Total_Hours'] #creates the dependent variable we are trying to measure\n",
    "Foods2.replace([np.inf, -np.inf], np.nan, inplace=True) #converting all infinites to na's. will want to come up with a better solution later \n",
    "Foods2 = Foods2.dropna() #drops all na's for now\n",
    "\n",
    "weekdays = {0: \"Monday\",\n",
    "            1: \"Tuesday\",\n",
    "            2: \"Wednesday\",\n",
    "            3: \"Thursday\",\n",
    "            4: \"Friday\",\n",
    "            5: \"Saturday\",\n",
    "            6: \"Sunday\"}\n",
    "\n",
    "months =   {1: \"January\",\n",
    "            2: \"February\",\n",
    "            3: \"March\",\n",
    "            4: \"April\",\n",
    "            5: \"May\",\n",
    "            6: \"June\",\n",
    "            7: \"July\",\n",
    "            8: \"August\",\n",
    "            9: \"September\",\n",
    "            10: \"October\",\n",
    "            11: \"November\",\n",
    "            12: \"December\"}\n",
    "            \n",
    "Foods2['weekday'] = Foods2['Date'].apply(lambda x: weekdays[x.weekday()]) #adds the weekday of the particular entry \n",
    "Foods2['month'] = Foods2['Date'].apply(lambda x: months[x.month]) #add the reporting_date month as a separate variable\n",
    "\n",
    "\n",
    "#adds the sae data to the main dataset\n",
    "sae = pd.read_csv(DATA_FOLDER + '/sae.csv')\n",
    "\n",
    "Foods3 = Foods2.merge(sae, how='outer', on='BRNCH_CD')\n",
    "Foods3.to_csv(DATA_FOLDER + '/master_dataset.csv', index=False)\n",
    "\n",
    "#Foods3.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
