{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sys import path\n",
    "path.append(\"..\")\n",
    "from config import *\n",
    "\n",
    "\n",
    "random_seed = 46\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "jobs = -1 #this is the number of cores that the models and test will run on. -1 means that all cores will be used \n",
    "\n",
    "#dropping all of the observations that are very likely errors\n",
    "dataset = pd.read_csv(DATA_FOLDER + \"/master_dataset.csv\")\n",
    "dataset = dataset[dataset[\"cases_hrs\"] <= 300]\n",
    "dataset = dataset[dataset[\"Total_Hours\"] >= 10]\n",
    "\n",
    "\n",
    "#dropping all of the uneeded columns\n",
    "to_drop = [\"Date\", \"Total_Hours\", \"Total_Cases\", \"B_HrsPct\", \"B_Cases\", \"Total_Each_Day\", \"dry_ratio\", \"clr_ratio\", \"frz_ratio\", \"GO_LIVE_DATE\", \"LABEL_TYPE\"]\n",
    "dataset_build = dataset.drop(labels=to_drop, axis=1)\n",
    "dataset_build.rename(columns={\"BRNCH_CD\":\"brnch_cd\", \"A_HrsPct\":\"a_hrspct\", \"C_HrsPct\":\"c_hrspct\", \"A_Cases\":\"a_cases\", \"C_Cases\":\"c_cases\"}, inplace=True)\n",
    "dataset_build.to_csv(DATA_FOLDER + \"/model_dataset.csv\",index=False)\n",
    "\n",
    "numeric_features = [\"a_hrspct\",  'c_hrspct', 'a_cases', 'c_cases']\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())]\n",
    ")\n",
    "categorical_features=['brnch_cd', 'weekday', 'month']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_build.drop(labels=\"cases_hrs\", axis=1),dataset_build['cases_hrs'], random_state=random_seed, train_size = .70)\n",
    "\n",
    "scores = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#linear regression\n",
    "reg_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", LinearRegression())]\n",
    ")\n",
    "reg_pipe.fit(X_train, y_train)\n",
    "scores.append([\"Reg\", reg_pipe.score(X_test, y_test)])\n",
    "\n",
    "#gradient boosting regressor model\n",
    "gbr_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(random_state=random_seed))]\n",
    ")\n",
    "gbr_pipe.fit(X_train, y_train)\n",
    "scores.append([\"GBR Init\", gbr_pipe.score(X_test, y_test)])\n",
    "\n",
    "#improved gbr\n",
    "gbr_improved_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, n_estimators=1000, learning_rate=0.3, max_depth=10, random_state=random_seed, loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved\", gbr_improved_pipe.score(X_test, y_test)])\n",
    "\n",
    "#improved gbr 2.0 - lower depth\n",
    "gbr_improved_pipe_2 = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, n_estimators=1000, learning_rate=0.3, max_depth=5, random_state=random_seed, loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe_2.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved\", gbr_improved_pipe_2.score(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GBR Grid Search\n",
    "* Takes a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gbrgrid = {'n_estimators':[500,1000,2000], \n",
    "           'learning_rate':[0.15,0.3], \n",
    "           'max_depth':[5,10]}\n",
    "\n",
    "crossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\n",
    "\n",
    "gbrgrid_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"gbrsearch\", GridSearchCV(estimator = GradientBoostingRegressor(), \n",
    "                                                                      param_grid = gbrgrid,\n",
    "                                                                      scoring = 'neg_mean_squared_error',\n",
    "                                                                      verbose = 1,\n",
    "                                                                      cv = crossvalidation, n_jobs=jobs))]\n",
    ")\n",
    "gbrsearch = gbrgrid_pipe.fit(X_train, y_train)\n",
    "gbrgrid_pipe.best_params_, gbrgrid_pipe.best_score_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        3672.8376            6.79m\n",
      "         2        3634.7779            6.64m\n",
      "         3        3600.4718            6.71m\n",
      "         4        3566.9995            6.62m\n",
      "         5        3536.5007            6.57m\n",
      "         6        3508.7512            6.56m\n",
      "         7        3484.6493            6.55m\n",
      "         8        3459.9013            6.51m\n",
      "         9        3438.6136            6.52m\n",
      "        10        3415.3945            6.55m\n",
      "        20        3259.2275            6.51m\n",
      "        30        3164.2132            6.43m\n",
      "        40        3104.2060            6.35m\n",
      "        50        3065.1080            6.19m\n",
      "        60        3025.0354            5.78m\n",
      "        70        2993.7219            5.41m\n",
      "        80        2964.4815            5.14m\n",
      "        90        2938.8806            4.92m\n",
      "       100        2916.1085            4.73m\n",
      "       200        2733.5650            3.78m\n",
      "       300        2600.1975            3.34m\n",
      "       400        2485.2323            3.04m\n",
      "       500        2387.4672            2.79m\n",
      "       600        2292.3107            2.57m\n",
      "       700        2210.0009            2.37m\n",
      "       800        2136.5710            2.17m\n",
      "       900        2071.3199            1.98m\n",
      "      1000        2004.5392            1.79m\n",
      "      2000        1538.2919            0.00s\n"
     ]
    }
   ],
   "source": [
    "#improved gbr\n",
    "    ## to reduce variablity, depth for gbr should be between 4 - 8\n",
    "    ## n_estimators = 2000\n",
    "    ## smaller learning rate = 0.05 - 0.15 is good\n",
    "    ## 4 minutes\n",
    "gbr_improved_pipe_3 = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, \n",
    "                                                                               n_estimators=2000, \n",
    "                                                                               learning_rate=0.05,\n",
    "                                                                               max_depth=6,  \n",
    "                                                                               loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe_3.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved 3\",gbr_improved_pipe_3.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Trees Regressor\n",
    "* Best MSE / RMSE = 242.242 / 15.564\n",
    "    * With DEFAULT paramters, performs pretty well against tuned GBR\n",
    "    * increasing n_estimators does not change much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   26.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#Extra trees base model, 5 1/2 minutes\n",
    "xtratree_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", ExtraTreesRegressor(verbose=1, n_jobs = jobs))]\n",
    ")\n",
    "xtratree_pipe.fit(X_train, y_train)\n",
    "scores.append([\"xtratree\",xtratree_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "* Best MSE / RMSE = 261.028 / 16.156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest grid search\n",
    "## run if needed\n",
    "\"\"\"\n",
    "crossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\n",
    "randfor_param = {\n",
    "             'max_depth': [5, 10, 15],\n",
    "             'n_estimators': [500, 1000, 1500]}\n",
    "randfor_search = GridSearchCV(RandomForestRegressor(n_jobs=jobs), randfor_param, refit = True, verbose = 3, cv = crossvalidation, scoring = 'neg_mean_squared_error', n_jobs=jobs)\n",
    "randfor_search.fit(X_train, y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   46.6s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#baseline RFR, outperformed by extratrees regressor - 4 min\n",
    "randfor_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", RandomForestRegressor(verbose=1, n_jobs=jobs))]\n",
    ")\n",
    "randfor_pipe.fit(X_train, y_train)\n",
    "scores.append([\"RFR\", randfor_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressor\n",
    "* Best MSE / RMSE = 466.094 / 21.589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#baseline Decision tree regressor\n",
    "dectree_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", DecisionTreeRegressor())]\n",
    ")\n",
    "dectree_pipe.fit(X_train, y_train)\n",
    "scores.append([\"Decision Tree\", dectree_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Reg', 0.050882787387249384],\n",
       " ['GBR Init', 0.1376223526075423],\n",
       " ['GBR Improved', -0.08274349859880914],\n",
       " ['GBR Improved', 0.04058212754023327],\n",
       " ['GBR Improved 3', 0.11209127690946474],\n",
       " ['xtratree', 0.03662133309109494],\n",
       " ['RFR', 0.08079112462153282],\n",
       " ['Decision Tree', -0.6565450546367695]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gbr_improved_pipe_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m dump(gbr_improved_pipe, MODEL_FOLDER \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/gbr_improved.joblib\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m dump(gbr_improved_pipe_2, MODEL_FOLDER \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/gbr_improved_2.joblib\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m dump(gbr_improved_pipe_3, MODEL_FOLDER \u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/gbr_improved_3.joblib\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m dump(randfor_pipe, MODEL_FOLDER \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/rfr.joblib\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m dump(dectree_pipe, MODEL_FOLDER \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/dec_tree.joblib\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gbr_improved_pipe_3' is not defined"
     ]
    }
   ],
   "source": [
    "#pickling the model \n",
    "from joblib import dump\n",
    "\n",
    "dump(reg_pipe, MODEL_FOLDER + \"/lin_reg.joblib\") \n",
    "dump(gbr_pipe, MODEL_FOLDER + \"/gbr_init.joblib\")\n",
    "dump(gbr_improved_pipe, MODEL_FOLDER + \"/gbr_improved.joblib\")\n",
    "dump(gbr_improved_pipe_2, MODEL_FOLDER + \"/gbr_improved_2.joblib\")\n",
    "dump(gbr_improved_pipe_3, MODEL_FOLDER +\"/gbr_improved_3.joblib\")\n",
    "dump(randfor_pipe, MODEL_FOLDER + \"/rfr.joblib\")\n",
    "dump(dectree_pipe, MODEL_FOLDER + \"/dec_tree.joblib\")\n",
    "dump(xtratree_pipe, MODEL_FOLDER + \"/xtra_tree.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
